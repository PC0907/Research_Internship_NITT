{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a7219b-a209-4838-bfd1-67de2c231668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Label Map:\n",
      "{'O': 0, 'B-Corporation': 1, 'B-Locale': 2, 'B-Application': 3, 'B-Malware': 4, 'B-Vulnerability': 5, 'B-Assault_technique': 6, 'B-Infected_file': 7, 'B-APT_group': 8, 'B-Ransomware': 9, 'B-Campaign': 10, 'B-Algorithm': 11, 'B-System': 12, 'B-Organization': 13, 'B-Indicator': 14, 'I-Corporation': 15, 'I-Locale': 16, 'I-Application': 17, 'I-Malware': 18, 'I-Vulnerability': 19, 'I-Assault_technique': 20, 'I-Infected_file': 21, 'I-APT_group': 22, 'I-Ransomware': 23, 'I-Campaign': 24, 'I-Algorithm': 25, 'I-System': 26, 'I-Organization': 27, 'I-Indicator': 28, 'E-Corporation': 29, 'E-Locale': 30, 'E-Application': 31, 'E-Malware': 32, 'E-Vulnerability': 33, 'E-Assault_technique': 34, 'E-Infected_file': 35, 'E-APT_group': 36, 'E-Ransomware': 37, 'E-Campaign': 38, 'E-Algorithm': 39, 'E-System': 40, 'E-Organization': 41, 'E-Indicator': 42, 'S-Corporation': 43, 'S-Locale': 44, 'S-Application': 45, 'S-Malware': 46, 'S-Vulnerability': 47, 'S-Assault_technique': 48, 'S-Infected_file': 49, 'S-APT_group': 50, 'S-Ransomware': 51, 'S-Campaign': 52, 'S-Algorithm': 53, 'S-System': 54, 'S-Organization': 55, 'S-Indicator': 56, '<start>': 57, '<end>': 58}\n",
      "\n",
      "Relation Label Map:\n",
      "{'O': 0, 'B-originate_from': 1, 'B-utilizes': 2, 'B-is_susceptible': 3, 'B-contains_product': 4, 'B-contains_file': 5, 'B-belongs_to': 6, 'B-operates_in': 7, 'B-launches': 8, 'B-is_a_type_of': 9, 'B-indicates': 10, 'B-implements': 11, 'I-originate_from': 12, 'I-utilizes': 13, 'I-is_susceptible': 14, 'I-contains_product': 15, 'I-contains_file': 16, 'I-belongs_to': 17, 'I-operates_in': 18, 'I-launches': 19, 'I-is_a_type_of': 20, 'I-indicates': 21, 'I-implements': 22, 'E-originate_from': 23, 'E-utilizes': 24, 'E-is_susceptible': 25, 'E-contains_product': 26, 'E-contains_file': 27, 'E-belongs_to': 28, 'E-operates_in': 29, 'E-launches': 30, 'E-is_a_type_of': 31, 'E-indicates': 32, 'E-implements': 33, 'S-originate_from': 34, 'S-utilizes': 35, 'S-is_susceptible': 36, 'S-contains_product': 37, 'S-contains_file': 38, 'S-belongs_to': 39, 'S-operates_in': 40, 'S-launches': 41, 'S-is_a_type_of': 42, 'S-indicates': 43, 'S-implements': 44}\n",
      "\n",
      "Train Data Shapes:\n",
      "Input IDs: torch.Size([2811, 512])\n",
      "Attention Masks: torch.Size([2811, 512])\n",
      "Label IDs: torch.Size([2811, 512])\n",
      "\n",
      "Total number of labels: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForTokenClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define entity and relation types\n",
    "entity_types = [\"Corporation\", \"Locale\", \"Application\", \"Malware\", \"Vulnerability\", \"Assault_technique\",\n",
    "                \"Infected_file\", \"APT_group\", \"Ransomware\", \"Campaign\", \"Algorithm\", \"System\", \"Organization\", \"Indicator\"]\n",
    "\n",
    "relation_types = [\"originate_from\", \"utilizes\", \"is_susceptible\", \"contains_product\", \"contains_file\",\n",
    "                  \"belongs_to\", \"operates_in\", \"launches\", \"is_a_type_of\", \"indicates\", \"implements\"]\n",
    "\n",
    "# Define label mappings\n",
    "entity_tags = [\"O\"] + [f\"B-{entity}\" for entity in entity_types] + [f\"I-{entity}\" for entity in entity_types] + [f\"E-{entity}\" for entity in entity_types] + [f\"S-{entity}\" for entity in entity_types]\n",
    "relation_tags = [\"O\"] + [f\"B-{relation}\" for relation in relation_types] + [f\"I-{relation}\" for relation in relation_types] + [f\"E-{relation}\" for relation in relation_types] + [f\"S-{relation}\" for relation in relation_types]\n",
    "\n",
    "entity_label_map = {tag: idx for idx, tag in enumerate(entity_tags)}\n",
    "relation_label_map = {tag: idx for idx, tag in enumerate(relation_tags)}\n",
    "\n",
    "# Add start and end tokens to entity_label_map\n",
    "entity_label_map['<start>'] = len(entity_label_map)\n",
    "entity_label_map['<end>'] = len(entity_label_map)\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_ner_data(file_path, encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if sentence:\n",
    "                sentences.append(['<start>'] + sentence + ['<end>'])\n",
    "                labels.append(['<start>'] + label + ['<end>'])\n",
    "                sentence = []\n",
    "                label = []\n",
    "        else:\n",
    "            word, tag = line.strip().split('\\t')\n",
    "            sentence.append(word)\n",
    "            label.append(tag)\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(['<start>'] + sentence + ['<end>'])\n",
    "        labels.append(['<start>'] + label + ['<end>'])\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Preprocess data\n",
    "train_sentences, train_labels = preprocess_ner_data(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\nitt\\\\data\\\\MITREtrain.txt\")\n",
    "valid_sentences, valid_labels = preprocess_ner_data(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\nitt\\\\data\\\\MITREvalid.txt\")\n",
    "test_sentences, test_labels = preprocess_ner_data(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\nitt\\\\data\\\\MITREtest.txt\")\n",
    "\n",
    "print(\"Entity Label Map:\")\n",
    "print(entity_label_map)\n",
    "print(\"\\nRelation Label Map:\")\n",
    "print(relation_label_map)\n",
    "\n",
    "# Initialize the ELECTRA tokenizer\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# Function to tokenize and encode sentences\n",
    "def tokenize_and_encode(sentences, labels, tokenizer, entity_label_map, max_length=512):\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        # Tokenize each word separately\n",
    "        tokenized_words = [tokenizer.tokenize(word) for word in sentence]\n",
    "        \n",
    "        # Flatten the list of tokenized words\n",
    "        tokens = [token for word in tokenized_words for token in word]\n",
    "        \n",
    "        # Convert tokens to ids\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        \n",
    "        # Align labels\n",
    "        label_id = []\n",
    "        for word, word_tokens in zip(label, tokenized_words):\n",
    "            label_id.extend([entity_label_map.get(word, entity_label_map[\"O\"])] * len(word_tokens))\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(input_id) < max_length:\n",
    "            padding_length = max_length - len(input_id)\n",
    "            input_id = input_id + [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            label_id = label_id + [entity_label_map['<pad>']] * padding_length\n",
    "        else:\n",
    "            input_id = input_id[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            label_id = label_id[:max_length]\n",
    "        \n",
    "        all_input_ids.append(input_id)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_label_ids.append(label_id)\n",
    "\n",
    "    # Convert to tensors\n",
    "    all_input_ids = torch.tensor(all_input_ids)\n",
    "    all_attention_masks = torch.tensor(all_attention_masks)\n",
    "    all_label_ids = torch.tensor(all_label_ids)\n",
    "    \n",
    "    return all_input_ids, all_attention_masks, all_label_ids\n",
    "\n",
    "# Add padding token to entity_label_map\n",
    "entity_label_map['<pad>'] = len(entity_label_map)\n",
    "\n",
    "# Tokenize with adjusted max_length\n",
    "train_inputs, train_masks, train_labels = tokenize_and_encode(train_sentences, train_labels, tokenizer, entity_label_map, max_length=512)\n",
    "valid_inputs, valid_masks, valid_labels = tokenize_and_encode(valid_sentences, valid_labels, tokenizer, entity_label_map, max_length=512)\n",
    "test_inputs, test_masks, test_labels = tokenize_and_encode(test_sentences, test_labels, tokenizer, entity_label_map, max_length=512)\n",
    "\n",
    "# Create TensorDataset instances\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "train_batch_size = 8\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=train_batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=train_batch_size)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"\\nTrain Data Shapes:\")\n",
    "print(\"Input IDs:\", train_inputs.shape)\n",
    "print(\"Attention Masks:\", train_masks.shape)\n",
    "print(\"Label IDs:\", train_labels.shape)\n",
    "\n",
    "print(f\"\\nTotal number of labels: {len(entity_label_map)}\")\n",
    "\n",
    "# Initialize ELECTRA model for token classification\n",
    "model = ElectraForTokenClassification.from_pretrained('google/electra-base-discriminator', num_labels=len(entity_label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c1e733-f0bd-4fe9-9d85-6e672b219d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_loader, entity_label_map):\n",
    "    label_counts = {label: 0 for label in entity_label_map.keys()}\n",
    "    total_labels = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        labels = batch[2]  # Assuming labels are the third item in the batch\n",
    "        for label_seq in labels:\n",
    "            for label in label_seq:\n",
    "                label_name = list(entity_label_map.keys())[list(entity_label_map.values()).index(label.item())]\n",
    "                label_counts[label_name] += 1\n",
    "                total_labels += 1\n",
    "    \n",
    "    # Handle labels not present in the training data\n",
    "    min_count = 1  # To avoid division by zero\n",
    "    for label in label_counts:\n",
    "        if label_counts[label] == 0:\n",
    "            label_counts[label] = min_count\n",
    "            total_labels += min_count\n",
    "    \n",
    "    class_weights = {label: total_labels / (len(entity_label_map) * count) for label, count in label_counts.items()}\n",
    "    \n",
    "    # Normalize weights\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {label: min(weight / max_weight, 10.0) for label, weight in class_weights.items()}\n",
    "    \n",
    "    print(\"Class weights:\")\n",
    "    for label, weight in class_weights.items():\n",
    "        print(f\"{label}: {weight:.4f}\")\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ef922c-ebe9-4a1d-ace6-259ca67a63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights:\n",
      "B-Indicator: 0.6497\n",
      "B-Malware: 4.8730\n",
      "B-Organization: 19.5034\n",
      "B-System: 7.2310\n",
      "B-Vulnerability: 50.6613\n",
      "I-Indicator: 3.1188\n",
      "I-Malware: 35.2053\n",
      "I-Organization: 67.5484\n",
      "I-System: 14.9433\n",
      "I-Vulnerability: 95.4995\n",
      "O: 0.1162\n",
      "<start>: 0.0000\n",
      "<end>: 0.0000\n",
      "<pad>: 0.0000\n",
      "\n",
      "Class distribution:\n",
      "<end>: 8433 (0.59%)\n",
      "<pad>: 1330973 (92.48%)\n",
      "<start>: 8433 (0.59%)\n",
      "B-Indicator: 12788 (0.89%)\n",
      "B-Malware: 1705 (0.12%)\n",
      "B-Organization: 426 (0.03%)\n",
      "B-System: 1149 (0.08%)\n",
      "B-Vulnerability: 164 (0.01%)\n",
      "I-Indicator: 2664 (0.19%)\n",
      "I-Malware: 236 (0.02%)\n",
      "I-Organization: 123 (0.01%)\n",
      "I-System: 556 (0.04%)\n",
      "I-Vulnerability: 87 (0.01%)\n",
      "O: 71495 (4.97%)\n"
     ]
    }
   ],
   "source": [
    "# Before your training loop\n",
    "class_weights = calculate_class_weights(train_loader, entity_label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0e6236-1008-471e-a9cf-2cf88100ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with num_labels: 60\n"
     ]
    }
   ],
   "source": [
    "from TorchCRF import CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class JointExtractionModel(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_size, num_layers, device):\n",
    "        super(JointExtractionModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.electra = ElectraModel.from_pretrained('google/electra-base-discriminator').to(device)\n",
    "        self.gru = nn.GRU(768, hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_size * 2, num_labels).to(device)\n",
    "        self.crf = CRF(num_labels).to(device)\n",
    "        print(f\"Model initialized with num_labels: {num_labels}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        electra_output = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = electra_output.last_hidden_state\n",
    "        gru_output, _ = self.gru(hidden)\n",
    "        logits = self.linear(gru_output)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, input_ids, attention_mask, tags):\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        log_likelihood = self.crf.forward(logits, tags, mask=attention_mask.bool())\n",
    "        return -log_likelihood.mean()\n",
    "\n",
    "    def decode(self, input_ids, attention_mask):\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        return self.crf.viterbi_decode(logits, mask=attention_mask.bool())\n",
    "\n",
    "# Initialize the model\n",
    "hidden_size = 256  # or any other value you prefer\n",
    "num_layers = 2  # or any other value you prefer\n",
    "num_labels = len(entity_label_map)\n",
    "model = JointExtractionModel(num_labels, hidden_size, num_layers, device)\n",
    "\n",
    "# Initialize the ELECTRA tokenizer\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# The rest of your code (data loading, training loop, etc.) remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653d8d58-3656-4c3f-8003-62c7980df2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, val_loader, test_loader, entity_label_map, device, epochs=10, lr=2e-5, max_grad_norm=1.0, warmup_steps=0, patience=3):\n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(train_loader, entity_label_map)\n",
    "    class_weight_tensor = torch.tensor([class_weights[label] for label in entity_label_map.keys()]).to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    # Initialize best model tracking\n",
    "    best_val_f1 = 0\n",
    "    best_model = None\n",
    "    \n",
    "    # Early stopping variables\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate weighted loss\n",
    "            loss = model.crf(logits, labels, mask=attention_mask.bool(), reduction='none')\n",
    "            weighted_loss = (loss * class_weight_tensor[labels]).mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item()\n",
    "            progress_bar.set_postfix({'loss': f\"{weighted_loss.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_f1, val_precision, val_recall = evaluate(model, val_loader, entity_label_map, device)\n",
    "        print(f\"Validation - F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, 'electra_new_cw.pt')\n",
    "            print(\"New best model saved!\")\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load('electra_new_cw.pt'))\n",
    "    test_f1, test_precision, test_recall = evaluate(model, test_loader, entity_label_map, device)\n",
    "    print(f\"Test - F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54640c0c-73cd-4c64-994b-38887af8a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, entity_label_map, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    inv_label_map = {v: k for k, v in entity_label_map.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, tags = [b.to(device) for b in batch]\n",
    "            predictions = model.decode(input_ids, attention_mask)\n",
    "            \n",
    "            for pred, true, mask in zip(predictions, tags, attention_mask):\n",
    "                pred = [inv_label_map[p] for p, m in zip(pred, mask) if m.item() == 1]\n",
    "                true = [inv_label_map[t.item()] for t, m in zip(true, mask) if m.item() == 1 and t.item() != -100]\n",
    "                \n",
    "                # Ensure pred and true have the same length\n",
    "                min_len = min(len(pred), len(true))\n",
    "                pred = pred[:min_len]\n",
    "                true = true[:min_len]\n",
    "                \n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(true)\n",
    "\n",
    "    # Print some debug information\n",
    "    print(f\"Number of predicted sequences: {len(all_preds)}\")\n",
    "    print(f\"Number of true label sequences: {len(all_labels)}\")\n",
    "    print(f\"Lengths of first few predicted sequences: {[len(p) for p in all_preds[:5]]}\")\n",
    "    print(f\"Lengths of first few true label sequences: {[len(t) for t in all_labels[:5]]}\")\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9627617-0d2f-4cc2-9eac-89c3e4531d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:\n",
      "O: 0.0000\n",
      "B-Corporation: 1.0000\n",
      "B-Locale: 1.0000\n",
      "B-Application: 1.0000\n",
      "B-Malware: 0.0006\n",
      "B-Vulnerability: 0.0061\n",
      "B-Assault_technique: 1.0000\n",
      "B-Infected_file: 1.0000\n",
      "B-APT_group: 1.0000\n",
      "B-Ransomware: 1.0000\n",
      "B-Campaign: 1.0000\n",
      "B-Algorithm: 1.0000\n",
      "B-System: 0.0009\n",
      "B-Organization: 0.0023\n",
      "B-Indicator: 0.0001\n",
      "I-Corporation: 1.0000\n",
      "I-Locale: 1.0000\n",
      "I-Application: 1.0000\n",
      "I-Malware: 0.0042\n",
      "I-Vulnerability: 0.0115\n",
      "I-Assault_technique: 1.0000\n",
      "I-Infected_file: 1.0000\n",
      "I-APT_group: 1.0000\n",
      "I-Ransomware: 1.0000\n",
      "I-Campaign: 1.0000\n",
      "I-Algorithm: 1.0000\n",
      "I-System: 0.0018\n",
      "I-Organization: 0.0081\n",
      "I-Indicator: 0.0004\n",
      "E-Corporation: 1.0000\n",
      "E-Locale: 1.0000\n",
      "E-Application: 1.0000\n",
      "E-Malware: 1.0000\n",
      "E-Vulnerability: 1.0000\n",
      "E-Assault_technique: 1.0000\n",
      "E-Infected_file: 1.0000\n",
      "E-APT_group: 1.0000\n",
      "E-Ransomware: 1.0000\n",
      "E-Campaign: 1.0000\n",
      "E-Algorithm: 1.0000\n",
      "E-System: 1.0000\n",
      "E-Organization: 1.0000\n",
      "E-Indicator: 1.0000\n",
      "S-Corporation: 1.0000\n",
      "S-Locale: 1.0000\n",
      "S-Application: 1.0000\n",
      "S-Malware: 1.0000\n",
      "S-Vulnerability: 1.0000\n",
      "S-Assault_technique: 1.0000\n",
      "S-Infected_file: 1.0000\n",
      "S-APT_group: 1.0000\n",
      "S-Ransomware: 1.0000\n",
      "S-Campaign: 1.0000\n",
      "S-Algorithm: 1.0000\n",
      "S-System: 1.0000\n",
      "S-Organization: 1.0000\n",
      "S-Indicator: 1.0000\n",
      "<start>: 0.0001\n",
      "<end>: 0.0001\n",
      "<pad>: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   0%|                                                                              | 0/352 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'reduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#model = JointExtractionModel(num_labels, hidden_size, num_layers, device)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_label_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, test_loader, entity_label_map, device, epochs, lr, max_grad_norm, warmup_steps, patience)\u001b[0m\n\u001b[0;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate weighted loss\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m*\u001b[39m class_weight_tensor[labels])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'reduction'"
     ]
    }
   ],
   "source": [
    "#model = JointExtractionModel(num_labels, hidden_size, num_layers, device)\n",
    "model = train(model, train_loader, valid_loader, test_loader, entity_label_map, device, epochs=10, lr=2e-5, patience=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
